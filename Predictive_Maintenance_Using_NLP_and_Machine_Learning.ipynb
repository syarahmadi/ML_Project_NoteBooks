{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "KbEFxk6S4MGP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Simulate Textual Maintenance Reports and Operational Data\n",
        "First, we simulate maintenance reports and sensor data. For simplicity, these examples will be basic but aim to capture the essence of real data:"
      ],
      "metadata": {
        "id": "6DM4G3xT3xRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Below we generate n=6 samples of synthetic data.\n",
        "# Each row in maintenance_reports is correlated with the same row of sensor_data_temp.\n",
        "\n",
        "# Simulate some maintenance reports with keywords indicating potential issues\n",
        "maintenance_reports = [\n",
        "    \"Routine check. No issues detected.\",\n",
        "    \"Reported overheating in the engine compartment.\",\n",
        "    \"Vibration levels higher than normal. Inspection recommended.\",\n",
        "    \"Leak detected in the cooling system.\",\n",
        "    \"Routine check. Minor wear on belts.\",\n",
        "    \"Unusual noise heard from the gearbox. Further analysis required.\"\n",
        "]\n",
        "\n",
        "# Simulate some operational sensor data (e.g., temperature readings over time for each report)\n",
        "sensor_data_temp = [\n",
        "    [21, 22, 20, 21, 20],  # Normal temperature readings\n",
        "    [30, 31, 29, 32, 33],  # Overheating\n",
        "    [22, 23, 21, 22, 24],  # Normal with slight variation\n",
        "    [25, 26, 27, 28, 29],  # Gradual increase indicating a leak or failure\n",
        "    [19, 20, 21, 20, 19],  # Normal wear conditions\n",
        "    [22, 30, 23, 25, 24]   # Unusual spike indicating possible issues\n",
        "]\n",
        "\n",
        "# Simulating labels based on the described issues (1 for potential failure, 0 for no issue)\n",
        "labels = np.array([0, 1, 1, 1, 0, 1])  # Placeholder for actual failure occurrence data"
      ],
      "metadata": {
        "id": "43NX0vRs3y6m"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Feature Engineering\n",
        "Let's extract features from both the textual maintenance reports using an LLM for semantic understanding and the operational sensor data for statistical insights.\n",
        "\n",
        "#### Textual Data Feature Engineering\n",
        "Using a BERT model, transforming reports into features:"
      ],
      "metadata": {
        "id": "HIixvCaP35OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def generate_embeddings(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    outputs = model(**inputs)\n",
        "    # Use the pooled output as it provides a fixed size embedding for variable length text\n",
        "    embeddings = outputs.pooler_output\n",
        "    return embeddings\n",
        "\n",
        "# Generate embeddings for each report\n",
        "report_embeddings = torch.stack([generate_embeddings(report) for report in maintenance_reports])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4QcuOId32Gt",
        "outputId": "e5858c78-92c4-486a-e27e-15c43a9a2d36"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Operational Data Feature Engineering\n",
        "For the operational sensor data, we compute statistical features such as mean and standard deviation to capture the behavior over time:"
      ],
      "metadata": {
        "id": "NvlnYnkh39Gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "op_features = []\n",
        "\n",
        "for readings in sensor_data_temp:\n",
        "    mean_temp = np.mean(readings)\n",
        "    std_temp = np.std(readings)\n",
        "    max_temp = np.max(readings)\n",
        "    min_temp = np.min(readings)\n",
        "    op_features.append([mean_temp, std_temp, max_temp, min_temp])\n",
        "\n",
        "op_features = np.array(op_features)\n"
      ],
      "metadata": {
        "id": "PtD4lUcR38V-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Combine Features and Prepare Dataset\n",
        "Combining the engineered features from textual and operational data:"
      ],
      "metadata": {
        "id": "sc74GSXd4R2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming operational features are stored in `op_features` as a NumPy array\n",
        "op_features_tensor = torch.tensor(op_features, dtype=torch.float32)  # Convert operational features to a tensor\n",
        "\n",
        "# Remove the singleton dimension from report_embeddings\n",
        "report_embeddings_squeezed = report_embeddings.squeeze(1)\n",
        "\n",
        "# Concatenate text embeddings with operational features\n",
        "combined_features = torch.cat((report_embeddings_squeezed, op_features_tensor), dim=1)"
      ],
      "metadata": {
        "id": "zmA33Cu_4FW0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the Dataset and Creating DataLoaders\n"
      ],
      "metadata": {
        "id": "whLnwMsW4gsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import torch\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "features_tensor = torch.tensor(combined_features, dtype=torch.float32)\n",
        "labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "# Splitting the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_tensor, labels_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating TensorDatasets\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "# Creating DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EtA3VEG4dTo",
        "outputId": "6ba33735-dad3-4515-de19-c57ae1448e95"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-befc020a327e>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  features_tensor = torch.tensor(combined_features, dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the LSTM Model\n",
        "Here's an LSTM model defined using PyTorch, tailored for binary classification (predicting failure: yes or no):"
      ],
      "metadata": {
        "id": "wFmIETD84mIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim=1, num_layers=1):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "        out = self.linear(lstm_out)\n",
        "        return out\n",
        "\n",
        "model = LSTMModel(input_dim=combined_features.shape[1], hidden_dim=50)"
      ],
      "metadata": {
        "id": "lzvmuYt14j3k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model\n",
        "Now, we implement the training loop including calculating loss and updating model parameters:"
      ],
      "metadata": {
        "id": "v29vu8l64vlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs.unsqueeze(1))  # Add a dimension [batch, time_step, features]\n",
        "            loss = criterion(outputs.squeeze(), labels)  # Squeeze output to match label shape\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "train_model(model, train_loader, criterion, optimizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUe9EQgW4vAY",
        "outputId": "3b7b2adf-7814-4678-908d-46f9e76b7806"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.720948338508606\n",
            "Epoch 2, Loss: 0.6558082699775696\n",
            "Epoch 3, Loss: 0.6038987636566162\n",
            "Epoch 4, Loss: 0.5697295665740967\n",
            "Epoch 5, Loss: 0.5493376851081848\n",
            "Epoch 6, Loss: 0.5312048196792603\n",
            "Epoch 7, Loss: 0.5160410404205322\n",
            "Epoch 8, Loss: 0.5033602714538574\n",
            "Epoch 9, Loss: 0.48871761560440063\n",
            "Epoch 10, Loss: 0.4730031490325928\n",
            "Epoch 11, Loss: 0.4580586850643158\n",
            "Epoch 12, Loss: 0.44498956203460693\n",
            "Epoch 13, Loss: 0.4326537251472473\n",
            "Epoch 14, Loss: 0.41813403367996216\n",
            "Epoch 15, Loss: 0.405367374420166\n",
            "Epoch 16, Loss: 0.39328253269195557\n",
            "Epoch 17, Loss: 0.379280686378479\n",
            "Epoch 18, Loss: 0.366548627614975\n",
            "Epoch 19, Loss: 0.3555910587310791\n",
            "Epoch 20, Loss: 0.3436276614665985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the Model\n",
        "To evaluate the model's performance on unseen data, we can use the test dataset:"
      ],
      "metadata": {
        "id": "xofNG5mj4yk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs.unsqueeze(1))\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            total_loss += loss.item()\n",
        "            predicted = torch.sigmoid(outputs).round()  # Applying sigmoid to get [0,1] range and rounding off\n",
        "            correct_predictions += (predicted.squeeze() == labels).sum().item()\n",
        "    avg_loss = total_loss / len(test_loader.dataset)\n",
        "    accuracy = correct_predictions / len(test_loader.dataset)\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "evaluate_model(model, test_loader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWMztS-H4o08",
        "outputId": "dfbd32b5-0159-40ff-b64d-364eb35016d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.2463, Accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate test predictions"
      ],
      "metadata": {
        "id": "M7M_xIXm7EEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "predicted = torch.tensor([])  # Initialize an empty tensor to store predictions\n",
        "\n",
        "with torch.no_grad():  # No need to track gradients during prediction\n",
        "    for inputs, _ in test_loader:  # We only need the inputs from the test_loader\n",
        "        inputs = inputs.unsqueeze(1)  # Add an extra dimension for LSTM\n",
        "        outputs = model(inputs)  # Get the model's predictions (logits)\n",
        "        probs = torch.sigmoid(outputs)  # Apply sigmoid to convert logits to probabilities\n",
        "        batch_predicted = probs.round()  # Round probabilities to get binary predictions\n",
        "        predicted = torch.cat((predicted, batch_predicted), dim=0)  # Aggregate predictions\n",
        "\n",
        "# Now you have `predicted` filled with your model's predictions\n"
      ],
      "metadata": {
        "id": "UYqlGjhO7Cia"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this case, Precision, Recall, and F1 Score are much more important than the regular accuracy metric and returning positive cases (failure, etc) is much more important.\n",
        "\n",
        "### Precision, Recall, and F1 Score\n",
        "\n",
        "Precision measures the accuracy of positive predictions. It’s the ratio of true positive predictions to the total number of positive predictions (including false positives).\n",
        "\n",
        "Recall (Sensitivity) measures the ability of the model to detect all actual positives. It’s the ratio of true positive predictions to the total actual positives (including false negatives).\n",
        "\n",
        "F1 Score provides a balance between precision and recall, offering a single metric to assess performance when both are important. It's the harmonic mean of precision and recall.\n",
        "\n",
        "### Calculating Metrics in PyTorch\n",
        "After evaluating the model, we calculate these metrics as follows:"
      ],
      "metadata": {
        "id": "0_w66XUt6pp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming `y_test` and `predicted` are available from your model's test phase\n",
        "# y_test: actual labels\n",
        "# predicted: model's predictions\n",
        "\n",
        "# Convert tensors to NumPy arrays for compatibility with scikit-learn metrics\n",
        "y_true_np = y_test.numpy()\n",
        "predicted_np = predicted.numpy().round()  # Assuming your model outputs probabilities that need rounding\n",
        "\n",
        "precision = precision_score(y_true_np, predicted_np)\n",
        "recall = recall_score(y_true_np, predicted_np)\n",
        "f1 = f1_score(y_true_np, predicted_np)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcJszMxu43QS",
        "outputId": "11d044ea-726c-4abc-f6db-8ba5bde2f330"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.5000\n",
            "Recall: 1.0000\n",
            "F1 Score: 0.6667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rHr2YSAV6x1M"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}